{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c7507c",
   "metadata": {},
   "source": [
    "### 1. 라이브러리, 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c468b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # current gpu seed\n",
    "    torch.cuda.manual_seed_all(seed) # All gpu seed\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # True로 하면 gpu에 적합한 알고리즘을 선택함.\n",
    "\n",
    "RANDOM_SEED = 1500\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# 데이터 load\n",
    "df_train = pd.read_csv(\"train.csv\") \n",
    "df_test = pd.read_csv(\"submission.csv\") \n",
    "\n",
    "# object 컬럼들 소문자로 변환\n",
    "def lowercase_strings(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].str.lower()\n",
    "    return df\n",
    "\n",
    "# 함수 호출\n",
    "df_train = lowercase_strings(df_train)\n",
    "df_test = lowercase_strings(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14f693",
   "metadata": {},
   "source": [
    "### 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6318d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'customer_country'와 'customer_country.1' 컬럼 동일하므로 후자 삭제\n",
    "df_train = df_train.drop('customer_country.1', axis=1)\n",
    "df_test = df_test.drop('customer_country.1', axis=1)\n",
    "\n",
    "# 'customer_country' 컬럼에서 '//'를 NaN으로 대체하고, 나라 이름으로 변환\n",
    "df_train['customer_country'] = df_train['customer_country'].replace('//', np.nan)\n",
    "df_test['customer_country'] = df_test['customer_country'].replace('//', np.nan)\n",
    "df_train['customer_country'] = df_train['customer_country'].str.split('/').str[-1].str.strip()\n",
    "df_test['customer_country'] = df_test['customer_country'].str.split('/').str[-1].str.strip()\n",
    "\n",
    "\n",
    "def preprocess_customer_country(df):\n",
    "    # 'customer_country' 소문자 변환 및 결측치 처리\n",
    "    df['customer_country'] = df['customer_country'].replace('//', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].str.split('/').str[-1].str.strip()\n",
    "\n",
    "    # 상세 주소명 처리\n",
    "    df['customer_country'] = df['customer_country'].replace('700 patroon creek blvdalbanyny12206', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('100 vestavia parkwaybirminghamal35216', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('1100 itbprovout84602', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('fl 33772', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('3000 montour church road', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('1380 enterprise dr', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('222 maxine dr', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('210 route 4 east fl 4', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('600 5th street', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('1100 itbprovout84602', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('ma 01851', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('il 60069', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('717 general booth blvdvirginia beach, va 23451, usa', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('2900 highway 280suite 250birminghamal35223', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('6601 carroll highlands rd', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('275 mishawum road', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('3804 w broadway st, ardmore, ok 73401, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('101 metlife way, cary, nc, 27513 – met1', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('delmar, ny 12054 united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace(\"via dell'informatica 10 - 37036 san martino buon albergo (veneto), italy\", 'italy')\n",
    "    df['customer_country'] = df['customer_country'].replace('richardson, texas, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('3 nasson avenue', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('oh 45215, usa', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('2877 prospect rd, fort lauderdale, fl 33309', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('810 n kingston dr peoria, il 61604-2145', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('mi 48827', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('1001 main st', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('152 bowdoin street', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('ca 91915-6002', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('300 east park drive', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('united states 14503.', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('mo 64108.', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('239 court st, brooklyn, ny 11201, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('1110 morse rd, columbus, ohio, 43229, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('ca 92618 united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('jeffersonville, in united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('united states 32901', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('2529 w busch blvd suite 1000, tampa, fl 33618, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('6564 headquarters drplanotx75051', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('28001 238th st, le claire, ia 52753, usa', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('grapevine, texas, united states united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('6398 college blvd, overland park, ks 66211, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('993 niagara ave, san diego, ca 92107 united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('via e. de amicis, 23 . 90044 carini (pa)', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('45 n 200 wwillardut84340', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('san francisco, ca 94128, united states', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('ironhorse customs llc 4443 genella way north las vegas, nv 89031', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('7700 west sunrise blvdplantationfl33322', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('230 highland ave, suite 531somervillema2143', np.nan)\n",
    "    df['customer_country'] = df['customer_country'].replace('5301 stevens creek blvdsanta claraca95051', np.nan)\n",
    "\n",
    "    # 겹치는 국가 병합\n",
    "    df['customer_country'] = df['customer_country'].replace('us', 'united states')\n",
    "    df['customer_country'] = df['customer_country'].replace('us', 'united states')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_customer_country(df_train)\n",
    "df_test = preprocess_customer_country(df_test)\n",
    "\n",
    "\n",
    "# 빈도수가 2 미만인 값과 빈 문자열을 other 대체\n",
    "value_counts = df_train['customer_country'].value_counts()\n",
    "values_to_replace = value_counts[value_counts < 2].index.tolist()\n",
    "values_to_replace.append('') \n",
    "\n",
    "df_train['customer_country'] = df_train['customer_country'].apply(lambda x: 'others' if x in values_to_replace else x)\n",
    "df_test['customer_country'] = df_test['customer_country'].apply(lambda x: 'others' if x in values_to_replace else x)\n",
    "\n",
    "\n",
    "def preprocess_business_unit(df):\n",
    "    replacement_dict = {\n",
    "        'as': \"air solution business division\",\n",
    "        'id': \"information display business division\",\n",
    "        'it': \"information technology business division\",\n",
    "        'cm': \"camera module business division\",\n",
    "        'solution': \"solution business division\",\n",
    "    }\n",
    "    df['business_unit'] = df['business_unit'].replace(replacement_dict)\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_business_unit(df_train)\n",
    "df_test = preprocess_business_unit(df_test)\n",
    "\n",
    "\n",
    "def preprocess_customer_type(df):\n",
    "    df['customer_type'] = df['customer_type'].replace(['end-customer', 'homeowner', 'home owner', 'end-user', 'commercial end-user'], 'end customer')\n",
    "    df['customer_type'] = df['customer_type'].replace(['specifier/ influencer', 'specifier / influencer', 'consultant', 'architect/consultant'], 'influencers')\n",
    "    df['customer_type'] = df['customer_type'].replace(['etc.', 'other'], 'others')\n",
    "    df['customer_type'] = df['customer_type'].replace(['software / solution provider'], 'software/solution provider')\n",
    "    df['customer_type'] = df['customer_type'].replace(['installer'], 'installer/contractor')\n",
    "    df['customer_type'] = df['customer_type'].replace(['dealer/distributor'], 'distributor')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = preprocess_customer_type(df_train)\n",
    "df_test = preprocess_customer_type(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# 대략적인 그룹화 기준 설정\n",
    "grouped_positions = {\n",
    "    'Executive': ['ceo', 'founder', 'c-level', 'president', 'vice president', 'director',\n",
    "                  'executive', 'manager', 'vp', 'the big boss', 'proprietário(a)', 'co-founder', 'chief executive officer',\n",
    "                  'principal & director', 'ceo/fundador', 'entrepreneurship', 'decision-maker',\n",
    "                  'c-levelexecutive', 'decision-influencer', 'leadership/executive office/owner',\n",
    "                  'decision influencer', 'decision maker', 'vicepresident', 'c-level executive',\n",
    "                  'ceo/founder', 'director'],\n",
    "    'Technical': ['engineer', 'technical', 'developer', 'architect', 'installer', 'técnico', 'engineering',\n",
    "                  'medical device manufacturer', 'manufacturer', 'software /solution provider',\n",
    "                  'it', 'information technology', 'sysadmin'],\n",
    "    'Sales_Marketing': ['sales', 'marketing', 'business development', 'business development/sales', 'subsidiary sales (ise)'],\n",
    "    'Consulting': ['consultant', 'advisor', 'commercial consultant', 'consulting', 'associate/analyst'],\n",
    "    'Education': ['teacher', 'professor', 'educator', 'lecturer', 'trainer', 'education professional',\n",
    "                  'associate professor', 'quantitative aptitude faculty', 'maths lecturer', 'senior lecturer',\n",
    "                  'education', 'neet/ olympiad expert faculty', 'associate professor in electronics engg',\n",
    "                  'asst prof.', 'professor of mathematics', 'physics and mathematics teacher',  'science teacher',\n",
    "                  'math and physics teacher', 'principal at oxford integrated pu science college', 'academic specialist',\n",
    "                  'prof.', 'physics teacher', 'assistant professor'],\n",
    "    'Administrative': ['administrative', 'secretary', 'assistant', 'professional trainer'],\n",
    "\n",
    "    'Healthcare': ['doctor', 'nurse', 'medical', 'healthcare', 'pathologist', 'surgery professional', 'tierarzt',\n",
    "                   'medical imaging specialist', 'hospital'],\n",
    "    'Intern' : ['intern', 'trainee', 'entry level', 'employee', 'entrylevel']\n",
    "}\n",
    "\n",
    "# 그룹화 함수 정의  -,/  공백으로 대체\n",
    "def preprocess_customer_position(position):\n",
    "    for group, keywords in grouped_positions.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in position.lower().replace('-', ' ').replace('/', ' '):\n",
    "                return group\n",
    "    return 'Others'\n",
    "\n",
    "# 모든 customer_position 범주에 대해 그룹화 실행\n",
    "df_train['customer_position'] = df_train['customer_position'].apply(preprocess_customer_position)\n",
    "df_test['customer_position'] = df_test['customer_position'].apply(preprocess_customer_position)\n",
    "\n",
    "\n",
    "def preprocess_inquiry_type(df):\n",
    "    df[\"inquiry_type\"] = df[\"inquiry_type\"].replace({\n",
    "        'request for quotation or purchase': 'quotation or purchase consultation',\n",
    "        'quotation_or_purchase_consultation': 'quotation or purchase consultation',\n",
    "        'purchase or quotation': 'quotation or purchase consultation',\n",
    "        'purchase': 'quotation or purchase consultation',\n",
    "        'technical consultation': 'usage or technical consultation',\n",
    "        'technical support': 'usage or technical consultation',\n",
    "        'request for technical consulting': 'usage or technical consultation',\n",
    "        'usage_or_technical_consultation': 'usage or technical consultation',\n",
    "        'technical': 'usage or technical consultation',\n",
    "        'technical_consultation': 'usage or technical consultation',\n",
    "        'sales': 'sales inquiry',\n",
    "        'etc.': 'other',\n",
    "        'others': 'other',\n",
    "        'other_': 'other'\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = preprocess_inquiry_type(df_train)\n",
    "df_test = preprocess_inquiry_type(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# 텍스트 정규화 함수 정의\n",
    "def normalize_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"  # NaN 값은 빈 문자열로 처리\n",
    "    text = text.lower()  # 소문자 변환\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # 특수 문자 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 불필요한 공백 제거\n",
    "    return text\n",
    "\n",
    "# 전체 규칙을 적용하는 함수 정의\n",
    "def apply_full_merge_rules(text):\n",
    "    full_merge_rules = {\n",
    "        r'details\\s+send': 'details shared',\n",
    "        r'details\\s+shared': 'details shared',\n",
    "        r'3\\s*months\\s*6\\s*months': '3 months - 6 months',\n",
    "        r'quote\\s+has\\s+been\\s+sent\\s+to\\s+customer': 'quote shared',\n",
    "        r'quotation\\s+shared': 'quote shared',\n",
    "        r'being\\s+followed\\s+up': 'following up',\n",
    "        r'following\\s+up': 'following up',\n",
    "        r'no\\s+response': 'no response',\n",
    "        r'not\\s+responding': 'no response',\n",
    "        r'budget\\s+problem': 'budget issue'\n",
    "    }\n",
    "    for pattern, replacement in full_merge_rules.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    full_others_rules = [\n",
    "        'not require', 'budget issue', 'duplicate lead', 'rnr', 'demo scheduled'\n",
    "    ]\n",
    "    for pattern in full_others_rules:\n",
    "        if re.search(pattern, text):\n",
    "            return 'others'\n",
    "    return text\n",
    "\n",
    "\n",
    "# 정규화된 값들에 대해 전체 처리 적용\n",
    "df_train['expected_timeline'] = df_train['expected_timeline'].apply(lambda x: apply_full_merge_rules(normalize_text(x)))\n",
    "df_test['expected_timeline'] = df_test['expected_timeline'].apply(lambda x: apply_full_merge_rules(normalize_text(x)))\n",
    "\n",
    "\n",
    "# 추가 정제 규칙에 따라 값을 변경하는 함수 정의\n",
    "def refine_expected_timeline(text):\n",
    "    refine_rules = {\n",
    "        r'lessthan3months': 'less than 3 months',\n",
    "        r'etc': 'others',\n",
    "        r'the client is not having any requirement hence closig in system although the details of idb are mailed to client': 'no requirement',\n",
    "        r'the client is not having any requirement he was only browsing through the produt hence closig in system although the details of idb are mailed to client': 'no requirement',\n",
    "        r'he client is not having any requirement hence closig in system although the details of idb are mailed to client': 'no requirement',\n",
    "        r\"didn't respond\": 'no response',\n",
    "        r'couldnt connect': 'no response',\n",
    "        r'not answering call': 'no response',\n",
    "        r'not answering call lead shared with rd': 'no response',\n",
    "        r'not reachable': 'no response',\n",
    "        r'tried to reach several times but no response': 'no response',\n",
    "        r'tried couple of times but he is no response we will try again': 'no response',\n",
    "        r'didnt respond': 'no response',\n",
    "        r'no response to calls': 'no response',\n",
    "        r'not answering call sales remarks tried to reach him multiple times but he is no response request to shailja to reconnect with customer we are dropping this lead for now': 'no response',\n",
    "        r'9months1year': '9 months 1 year',\n",
    "        r'forwarded to bdo following up': 'following up',\n",
    "        r'forwarded to bdo to followup': 'following up',\n",
    "        r'6months9months': '6 months 9 months',\n",
    "        r'morethanayear': 'more than a year',\n",
    "        r'less then 6 months': 'less than 6 months',\n",
    "        r'less than 5 months': 'less than 6 months',\n",
    "        r'dicsussed with clientdetails shared on mail client have no budgets to buy now hence closing in the system': 'low budget',\n",
    "        r'quote shared': 'quote send',\n",
    "        r'quote shared for ultra strothersh and 49vl5g he will check with management and update us': 'quote send',\n",
    "        }\n",
    "\n",
    "    for pattern, replacement in refine_rules.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# 예시 데이터에 함수 적용\n",
    "df_train['expected_timeline'] = df_train['expected_timeline'].apply(refine_expected_timeline)\n",
    "df_test['expected_timeline'] = df_test['expected_timeline'].apply(refine_expected_timeline)\n",
    "\n",
    "# ver_cus 컬럼, 조건을 만족하는 행의 ver_cus 값을 1로 업데이트\n",
    "df_train.loc[((df_train['business_area'].isin(['corporate/office', 'retail', 'education', 'hotel & accommodation'])) &\n",
    "              (df_train['customer_type'] == 'end customer')), 'ver_cus'] = 1\n",
    "\n",
    "df_test.loc[((df_test['business_area'].isin(['corporate/office', 'retail', 'education', 'hotel & accommodation'])) &\n",
    "              (df_test['customer_type'] == 'end customer')), 'ver_cus'] = 1\n",
    "\n",
    "\n",
    "# 해당 값들 리스트화\n",
    "product_category_list = df_train[df_train['ver_pro'] == 1]['product_category'].tolist()\n",
    "\n",
    "# 'product_category' 컬럼의 값이 'product_category_list'에 포함되고,\n",
    "# 'business_area' 컬럼의 값이 주어진 리스트 중 하나인 행의 'ver_pro' 값을 1로 업데이트\n",
    "df_train.loc[((df_train['business_area'].isin(['corporate / office', 'retail', 'hotel & accommodation'])) &\n",
    "              (df_train['product_category'].isin(product_category_list))), 'ver_pro'] = 1\n",
    "\n",
    "df_test.loc[((df_test['business_area'].isin(['corporate / office', 'retail', 'hotel & accommodation'])) &\n",
    "              (df_test['product_category'].isin(product_category_list))), 'ver_pro'] = 1\n",
    "\n",
    "\n",
    "# 사업 영역별 전환율: business_area 별로 전체 리드 중 전환된 리드의 비율 계산\n",
    "business_area_conversion_rate = df_train.groupby('business_area')['is_converted'].mean().reset_index()\n",
    "business_area_conversion_rate.rename(columns={'is_converted': 'business_area_conversion_rate'}, inplace=True)\n",
    "\n",
    "# 데이터셋에 사업 영역별 전환율을 매핑\n",
    "df_train = pd.merge(df_train, business_area_conversion_rate, on='business_area', how='left')\n",
    "df_test = pd.merge(df_test, business_area_conversion_rate, on='business_area', how='left')\n",
    "\n",
    "# 리드 소유자별 전환율: lead_owner 별로 각 영업 담당자가 관리한 리드 중 전환된 비율 계산\n",
    "lead_owner_conversion_rate = df_train.groupby('lead_owner')['is_converted'].mean().reset_index()\n",
    "lead_owner_conversion_rate.rename(columns={'is_converted': 'lead_owner_conversion_rate'}, inplace=True)\n",
    "\n",
    "# 데이터셋에 리드 소유자별 전환율을 매핑\n",
    "df_train = pd.merge(df_train, lead_owner_conversion_rate, on='lead_owner', how='left')\n",
    "df_test = pd.merge(df_test, lead_owner_conversion_rate, on='lead_owner', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9280cc9",
   "metadata": {},
   "source": [
    "### 3.텍스트 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a546415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_embed(df):\n",
    "    # Torch에서 GPU 지원 활성화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # SentenceTransformer 모델을 GPU로 이동\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "    # 문장 임베딩을 생성할 컬럼 목록\n",
    "    text_columns = ['customer_country', 'business_unit', 'customer_type', 'enterprise', 'customer_job', 'inquiry_type',\n",
    "                    'product_category', 'product_subcategory', 'product_modelname', 'customer_position', 'response_corporate',\n",
    "                    'expected_timeline', 'business_area', 'business_subarea']\n",
    "\n",
    "    for column in text_columns:\n",
    "        if column in df.columns:\n",
    "            # 해당 컬럼의 모든 텍스트를 리스트로 추출, NaN 값 처리\n",
    "            texts = df[column].fillna(\"\").astype(str).tolist()\n",
    "            # 문장 임베딩 생성\n",
    "            embeddings = model.encode(texts)\n",
    "            # 임베딩의 평균을 계산\n",
    "            embedding_means = np.mean(embeddings, axis=1)\n",
    "            # 새 컬럼 이름 정의\n",
    "            new_column_name = f\"{column}_embedding_mean\"\n",
    "            # 평균 임베딩 값을 새 컬럼으로 추가\n",
    "            df[new_column_name] = embedding_means\n",
    "\n",
    "    # 데이터프레임에서 특정 컬럼 드롭\n",
    "    df.drop(columns=text_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = preprocess_and_embed(df_train)\n",
    "df_test = preprocess_and_embed(df_test)\n",
    "\n",
    "# 'is_converted' 컬럼에서 True를 1로, False를 0으로 변환\n",
    "df_train['is_converted'] = df_train['is_converted'].astype(int)\n",
    "df_test['is_converted'] = df_test['is_converted'].astype(int)\n",
    "\n",
    "df_train.fillna(0, inplace=True)\n",
    "df_test.fillna(0, inplace=True)\n",
    "\n",
    "X = df_train.drop(\"is_converted\", axis=1)\n",
    "y = df_train[\"is_converted\"]\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=1500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39970d23",
   "metadata": {},
   "source": [
    "### 4. 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ec26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3084, number of negative: 3084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2201\n",
      "[LightGBM] [Info] Number of data points in the train set: 6168, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2115\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2121\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2097\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2468, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2111\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500101 -> initscore=0.000405\n",
      "[LightGBM] [Info] Start training from score 0.000405\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2468\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2098\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499899 -> initscore=-0.000405\n",
      "[LightGBM] [Info] Start training from score -0.000405\n",
      "[LightGBM] [Info] Number of positive: 3084, number of negative: 3084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2162\n",
      "[LightGBM] [Info] Number of data points in the train set: 6168, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2086\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2085\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2071\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2468, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2069\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500101 -> initscore=0.000405\n",
      "[LightGBM] [Info] Start training from score 0.000405\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2468\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2073\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499899 -> initscore=-0.000405\n",
      "[LightGBM] [Info] Start training from score -0.000405\n",
      "[LightGBM] [Info] Number of positive: 3084, number of negative: 3084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2217\n",
      "[LightGBM] [Info] Number of data points in the train set: 6168, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2124\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2111\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2121\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2468, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2117\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500101 -> initscore=0.000405\n",
      "[LightGBM] [Info] Start training from score 0.000405\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2468\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2116\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499899 -> initscore=-0.000405\n",
      "[LightGBM] [Info] Start training from score -0.000405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3084, number of negative: 3084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2187\n",
      "[LightGBM] [Info] Number of data points in the train set: 6168, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2086\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2097\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2468, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2078\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500101 -> initscore=0.000405\n",
      "[LightGBM] [Info] Start training from score 0.000405\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2468\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2101\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499899 -> initscore=-0.000405\n",
      "[LightGBM] [Info] Start training from score -0.000405\n",
      "[LightGBM] [Info] Number of positive: 3084, number of negative: 3084\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2202\n",
      "[LightGBM] [Info] Number of data points in the train set: 6168, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2112\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2108\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2098\n",
      "[LightGBM] [Info] Number of data points in the train set: 4934, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2468, number of negative: 2467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2097\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500101 -> initscore=0.000405\n",
      "[LightGBM] [Info] Start training from score 0.000405\n",
      "[LightGBM] [Info] Number of positive: 2467, number of negative: 2468\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2097\n",
      "[LightGBM] [Info] Number of data points in the train set: 4935, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499899 -> initscore=-0.000405\n",
      "[LightGBM] [Info] Start training from score -0.000405\n",
      "평균 평가 지표:\n",
      "평균 정확도: 0.9359390083989967\n",
      "평균 정밀도: 0.7797305272891955\n",
      "평균 재현율: 0.945864989899249\n",
      "평균 F1 스코어: 0.836553726541411\n",
      "1566\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "# 개별 모델 및 스태킹 앙상블 정의\n",
    "rf = RandomForestClassifier(random_state=6666)\n",
    "xgb = XGBClassifier(random_state=6666)\n",
    "cat = CatBoostClassifier(random_state=6666, verbose=0) # CatBoost의 출력을 억제하기 위해 verbose=0 추가\n",
    "lgbm = LGBMClassifier(random_state=6666)\n",
    "estimator_stacking = [('rf', rf), ('xgb', xgb), ('cat',cat), ('lgbm',lgbm)]\n",
    "stacking = StackingClassifier(estimators=estimator_stacking, final_estimator=rf)\n",
    "\n",
    "# 교차 검증을 위한 StratifiedKFold 설정\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=6666)\n",
    "\n",
    "# 교차 검증 및 모델 평가\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "rus = RandomUnderSampler(random_state=6666)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv.split(x_train, y_train)):\n",
    "    x_train_fold, x_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    x_train_res, y_train_res = rus.fit_resample(x_train_fold, y_train_fold)\n",
    "    \n",
    "    stacking.fit(x_train_res, y_train_res.ravel()) \n",
    "    y_pred = stacking.predict(x_val_fold)\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precision_scores.append(precision_score(y_val_fold, y_pred, average='macro')) \n",
    "    recall_scores.append(recall_score(y_val_fold, y_pred, average='macro')) \n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred, average='macro')) \n",
    "\n",
    "# 예측에 필요한 데이터 분리\n",
    "x_test = df_test.drop([\"is_converted\", \"id\"], axis=1)\n",
    "test_pred = stacking.predict(x_test)\n",
    "\n",
    "# 평가 지표의 평균값 출력\n",
    "print(\"평균 평가 지표:\")\n",
    "print(\"평균 정확도:\", np.mean(accuracy_scores))\n",
    "print(\"평균 정밀도:\", np.mean(precision_scores))\n",
    "print(\"평균 재현율:\", np.mean(recall_scores))\n",
    "print(\"평균 F1 스코어:\", np.mean(f1_scores))\n",
    "print(sum(test_pred)) # True로 예측된 개수 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b97dfa",
   "metadata": {},
   "source": [
    "### 5. 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b73171eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 데이터 읽어오기\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"is_converted\"] = test_pred\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwater",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
